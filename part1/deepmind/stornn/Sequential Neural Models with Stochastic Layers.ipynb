{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sequential Neural Models with Stochastic Layers](https://arxiv.org/abs/1605.07571)\n",
    "\n",
    "\n",
    "사전에 알면 좋은 것들\n",
    "* Latent Linear Models ( Murphy 12 )\n",
    "* State Space models and Kalman filter ( Murphy 18)\n",
    "* Hidden Markov Models ( Murphy 17 )\n",
    "* Variational Inference ( Murphy 21, Benjio 19 )\n",
    "* Variational Autoencoder ( Benjio 20.10.3 )\n",
    "* RNN ( Benjio 10 )\n",
    "* Variational RNN\n",
    "\n",
    "\n",
    "요약\n",
    "* 기존 RNN은 sequence 데이터를 잘 모델링하기는 하지만 variability가 좋지 못했다. \n",
    "* Autoencoder에 variability를 강화시킨 variational autoencoder의 approach를 받아들여서\n",
    "* RNN을 State Space Model과 융합하였다.\n",
    "* intractable 문제에 대해서는 variational inference로 대응하였다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN과 SSM의 비교\n",
    "\n",
    "State Space Model\n",
    "* 장점\n",
    "  * SSM 은 상태 천이가 확률 기반인지라 다양성(variability)를 더 나타낼 수 있다.\n",
    "* 단점\n",
    "  * 선형성의 한계, 장기 의존성 문제 해결 난망\n",
    "\n",
    "Recurrent Neural Network\n",
    "* 장점\n",
    "  * RNN 은 역전파로 쉽게 학습시키고 성과도 좋고 비선형성도 반영 가능, LSTM/GRU 등으로 장기 의존성 문제도 대응\n",
    "* 단점\n",
    "  * 상태 천이가 결정적(deterministic)여서 다양성(variability) 반영이 낮다.\n",
    "\n",
    "![](./images/snm/07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Recurrent Neural Networks ( Stacking RNN with nonlinear SSM )\n",
    "\n",
    "SSM 상태 천이는\n",
    "* 확률적(Stochastic)\n",
    "* 비선형적 ( by parameterized by neural network of RNN hidden states )\n",
    "* LSTM와 같은 특성을 가짐\n",
    "\n",
    "![](./images/snm/08.png)\n",
    "\n",
    "\n",
    "## 남은 문제는 어떻게 학습시킬 것인가?\n",
    "\n",
    "![](./images/snm/09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908v2.pdf)\n",
    "\n",
    "\n",
    "Generative Model\n",
    "* 고차원에서 변수끼리의 dependency를 알아내는 것, 구조를 이해하는 것\n",
    "* 기 수집된 데이터를 기반으로 해서 아직 관찰되지 않은(하지만 있음직한) 데이터를 자체로 생성하는 것\n",
    "\n",
    "\n",
    "Generating MNIST data\n",
    "* 잠재변수 0-9까지 중 하나를 선택한 후(sample from z), 필기체 그림(x)를 generate\n",
    "* 잠재변수와 모델 파리미터는 generate한 데이터가 Traning Data처럼 되게 학습되어야 한다.\n",
    "\n",
    "(그림)\n",
    "![](./images/snm/01.png)\n",
    "![](./images/snm/02.png)\n",
    "\n",
    "\n",
    "Variational AutoEncoder\n",
    "* 어떻게 잠재변수를 모델링할 것인가? \n",
    " * 그냥 N(0,I)로 하고 powerful한 function approximator를 사용해서 function을 학습하자.\n",
    "* 어떻게 Integral을 할 것인가?\n",
    "  * z에 대한 최대한 많은 sample들에 대해 P(X|z)를 구한 후 평균 => 굉장히 많은 sample 필요해서 비현실적\n",
    "  * 잠재변수 z의 space를 줄이는 것 => P(z|X), encoding\n",
    "    * P(z|X)를 구하는 것은 어려움, 대신 이와 최대한 가까운 Q(z|X)를 구하자 => Variational Inference\n",
    "\n",
    "Variational Inference 풀기\n",
    "* 데이터에 대한 정보를 최대화하는 것 \n",
    "* Q를 잘 선택해서 P(z|X)를 잘 근사화하는 것 => 풀기 어려움\n",
    "* stochastic gradient descent로 아래 식의 우측항을 최대화하는 것\n",
    "\n",
    "![](./images/snm/03.png)\n",
    "![](./images/snm/04.png)\n",
    "![](./images/snm/10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A Recurrent Latent Variable Model for Sequential Data]()\n",
    "\n",
    "Sequence data에 대한 generative model을 전통적으로 Dynamic Baysian network(DBN)이 효과적이었다\n",
    "* DBN의 예는 Kalman filter나 HMM이 있다.\n",
    "* DBN은 최근에 RNN 에 밀렸다. \n",
    "* RNN은 DBN에 비해 LSTM이나 GRU와 같은 rich한 internal state 표현력이 풍부하고 nonlineary를 가지는 장점은 있으나\n",
    "  * state의 천이가 결정적(deterministic)하다는 한계로 인해서 expressive power가 DBN보다는 떨어진다.\n",
    "* 그래서 RNN은 매우 구조화되고 variable한 데이터에는 적합하지 않다.\n",
    "  * be an inappropriate way to model the kind of variability observed in highly structured data,\n",
    "\n",
    "\n",
    "Variational RNN\n",
    "* Variational AE에서처럼 잠재변수를 도입해서 variability를 개선시키자.\n",
    "* 모든 timestamp마다 variational ae가 있는 셈\n",
    "\n",
    "![](./images/snm/06.png)\n",
    "\n",
    "## Generation\n",
    "\n",
    "* 학습만 되었다면 forward propagation은 쉬움\n",
    "\n",
    "![](./images/snm/11.png)\n",
    "![](./images/snm/12.png)\n",
    "![](./images/snm/13.png)\n",
    "\n",
    "## Inference\n",
    "\n",
    "* approximated\n",
    "\n",
    "![](./images/snm/14.png)\n",
    "\n",
    "## Learning\n",
    "\n",
    "\n",
    "![](./images/snm/15.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
